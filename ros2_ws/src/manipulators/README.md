# Manipulators

ROS2 package for Kinova Gen3 7-DOF torque control using Pinocchio for differential IK and gravity compensation. Includes object detection with a pluggable detector system, camera calibration tools, and a reactive pick-and-place policy. Communicates directly with the robot via Kortex API (no ros2_control drivers). Uses RealSense ROS2 driver for camera I/O.

## Architecture

```
                        ┌─────────────────────────────────────────────────────────┐
                        │                    pick_place_policy                    │
                        │              (FSM + velocity-limited motion)            │
                        └───────────────────────────┬─────────────────────────────┘
                                                    │
  /detected_object_point ───────────────────────────┤
                                                    │
                                                    ▼
keyboard_teleop ──/target_pose──> control_node ──torques──> Kinova (UDP @ 400Hz)
                  /gripper_cmd──┘      │
                                       │── /joint_states
                                       └── /ee_pose ────────> pick_place_policy

realsense2_camera ──/color/image_raw──> object_detection_node ──/detected_object_point──> pick_place_policy
                    /aligned_depth/──→         │
                    /camera_info────→          └── /detection_image
```

The control node runs a single controller (defined by launch file), with a fixed lifecycle:

```
Startup:  connect → clear faults → home → low-level servoing → torque mode → control loop
Shutdown: stop loop → position mode → high-level servoing → home → disconnect
```

## Package Structure

```
src/
├── hardware.py                  # KinovaHardware — Kortex TCP/UDP I/O + gripper
├── robot_model.py               # Pinocchio wrapper — gravity, Jacobian, FK (7-DOF reduced model)
├── diff_ik_controller.py        # Diff-IK + damped pseudoinverse + gravity comp → joint torques
├── control_node.py              # Main ROS2 node — startup/loop/shutdown
├── keyboard_teleop.py           # Keyboard → /target_pose + /gripper_command
├── object_detection_node.py     # ROS2 node — camera subscription, detection, 3D projection
├── pick_place_policy.py         # Reactive FSM pick-and-place policy
├── utility.py                   # Quaternion, rotation, pose error helpers
└── detectors/
    ├── base.py                  # DetectorBase protocol + Detection dataclass
    └── color_detector.py        # HSV color-based detection

scripts/
├── capture_images.py            # Capture calibration images from camera topic
├── calibrate_intrinsics.py      # Chessboard → camera matrix + distortion → YAML
└── calibrate_extrinsics.py      # Camera-to-robot transform → YAML

config/
├── kinova_gen3.yaml             # Robot + controller parameters
├── camera.yaml                  # Camera intrinsics + extrinsics (generated by scripts)
├── detection.yaml               # Detector type, HSV ranges, crop, depth range
└── pick_place.yaml              # Pick-place policy parameters (waypoints, velocities, timing)
```

## Usage

```bash
# Build
cd ~/manipulators/ros2_ws
colcon build --packages-select manipulators
source install/setup.bash

# Run with diff-IK + keyboard teleop
ros2 launch manipulators diff_ik.launch.py robot_ip:=192.168.1.10

# Run pick-place system (control + detection + policy)
ros2 launch manipulators pick_place.launch.py robot_ip:=192.168.1.10

# Start a pick-place cycle (from another terminal)
ros2 service call /pick_place/start std_srvs/srv/Trigger

# Abort pick-place if needed
ros2 service call /pick_place/abort std_srvs/srv/Trigger

# Emergency stop
ros2 service call /e_stop std_srvs/srv/Trigger
```

## Topics

### Control

| Topic | Type | Direction | Description |
|-------|------|-----------|-------------|
| `/target_pose` | `geometry_msgs/PoseStamped` | Input | Desired EE pose |
| `/gripper_command` | `std_msgs/Float64` | Input | Gripper target (0.0=open, 1.0=closed) |
| `/joint_states` | `sensor_msgs/JointState` | Output | Joint positions, velocities, torques |
| `/ee_pose` | `geometry_msgs/PoseStamped` | Output | Current EE pose |

### Detection

| Topic | Type | Direction | Description |
|-------|------|-----------|-------------|
| `/detected_object_point` | `geometry_msgs/PointStamped` | Output | Object position in robot frame |
| `/detection_image` | `sensor_msgs/Image` | Output | Annotated camera image |

## Services

| Service | Type | Description |
|---------|------|-------------|
| `/e_stop` | `std_srvs/Trigger` | Emergency stop — kills torque mode immediately |
| `/pick_place/start` | `std_srvs/Trigger` | Start pick-place cycle (requires fresh object detection) |
| `/pick_place/abort` | `std_srvs/Trigger` | Abort current operation, return to idle |

## Pick-Place Policy

The pick-place policy is a reactive finite state machine that:

1. **Waits at idle position** for a service trigger
2. **Approaches the object** with continuous tracking (reactive to object movement)
3. **Grasps** with configurable settle time
4. **Transports** through a mid-waypoint to the place position
5. **Places and releases** the object
6. **Returns to idle** for the next cycle

### State Machine

```
INIT → IDLE → APPROACH → DESCEND → GRASP → LIFT → MID_TRANSPORT → PRE_PLACE → PLACE_DESCEND → RELEASE → PLACE_ASCEND → RETURN_IDLE → IDLE
```

### Velocity Limiting

The policy uses velocity-limited target updates to create smooth motion:
- Target position moves at most `max_linear_velocity` per second
- Prevents sudden jumps when object is far away
- Creates natural acceleration/deceleration

### Configuration — `config/pick_place.yaml`

- `idle_position` — Where robot waits between cycles
- `mid_transport_position` — Waypoint during transport phase
- `pre_place_position` — Above place location (approach/retract point)
- `place_position` — Final place location
- `pre_grasp_height` — Height above object for approach
- `grasp_height` — Height offset for grasp (fingertip compensation)
- `lift_height` — Height above object after grasp
- `grasp_orientation` — Quaternion for end-effector (default: top-down)
- `max_linear_velocity` — Max target movement speed (m/s)
- `position_threshold` — Distance to consider "arrived"
- `grasp_settle_time` — Time to wait for gripper to close
- `release_settle_time` — Time to wait for gripper to open
- `detection_timeout` — Max age for "fresh" detection at trigger

## Configuration

### Control — `config/kinova_gen3.yaml`

- `robot_ip` — Kinova IP address
- `home_position_deg` — Joint angles for home position (degrees, Kinova 0-360 convention)
- `control_rate_hz` — Control loop frequency (default 400 Hz)
- `kp_task` — Task-space proportional gains `[x, y, z, rx, ry, rz]`
- `kp_joint` — Joint-space position gains (per joint, default 0.0 = disabled)
- `kd_joint` — Joint-space damping gains (per joint)
- `damping` — Pseudoinverse regularization
- `max_joint_velocity` — Safety clamp on diff-IK output (rad/s)
- `max_torque` — Per-joint torque limits (Nm)

### Detection — `config/detection.yaml`

- `detector_type` — Detector to use (`color`, or custom)
- `hsv_low` / `hsv_high` — HSV threshold range for color detector
- `bgr_low` / `bgr_high` — BGR pre-filter range
- `crop` — Region of interest `[x1, y1, x2, y2]` in pixels
- `min_area` — Minimum contour area to count as detection
- `depth_range` — Valid depth range in meters `[min, max]`

### Camera — `config/camera.yaml`

Generated by calibration scripts. Contains camera matrix, distortion coefficients, and camera-to-robot extrinsic transform (rvec/tvec).

## Keyboard Teleop Keys

```
W/S  — X forward/back        U/O — roll +/-
A/D  — Y left/right          I/K — pitch +/-
Q/E  — Z up/down             J/L — yaw +/-
G    — toggle gripper         ESC — quit
```

## Camera Calibration

See [docs/camera_calibration.md](docs/camera_calibration.md) for calibration guide.

```bash
# 1. Start RealSense camera
ros2 launch realsense2_camera rs_launch.py align_depth.enable:=true

# 2. Capture calibration images
ros2 run manipulators capture_images -- --output-dir ./calib_images --count 20

# 3. Compute intrinsics
python3 scripts/calibrate_intrinsics.py \
  --image-dir ./calib_images --output config/camera.yaml \
  --board-size 8 6 --square-size 25.0

# 4. Compute extrinsics
ros2 run manipulators calibrate_extrinsics -- \
  --camera-yaml config/camera.yaml --board-size 5 3 --square-size 45.0 \
  --board-to-robot-xyz -0.11 -0.01 0.0 --board-to-robot-rpy 0.0 180.0 0.0
```

## Adding a New Detector

1. Create `src/detectors/my_detector.py` implementing `detect(color_image) -> List[Detection]`
2. Register it in `src/detectors/__init__.py` DETECTORS dict
3. Set `detector_type: my_detector` in `config/detection.yaml`

## Adding a New Controller

1. Create `src/new_controller.py` with a `compute(target_pos, target_quat, q, dq) -> torques` method
2. Import and instantiate it in a copy of `control_node.py`
3. Add a new launch file `launch/new_controller.launch.py`

## Diagrams

Detailed mermaid diagrams are in `docs/diagrams/`:

- **[Control System](docs/diagrams/control.md)** — Control node architecture, startup/shutdown, control loop, e-stop, threading, hardware connections
- **[Detection System](docs/diagrams/detection.md)** — Detection pipeline, calibration workflow, coordinate transforms, pluggable detectors
- **[Pick-Place Policy](docs/diagrams/policy.md)** — State machine, velocity limiting, waypoints, service interface, system integration

## Dependencies

- ROS2 (Humble+)
- [Pinocchio](https://github.com/stack-of-tasks/pinocchio) — dynamics, Jacobians, FK
- [Kortex API](https://github.com/Kinovarobotics/kortex) — direct robot communication
- [realsense2_camera](https://github.com/IntelRealSense/realsense-ros) — RealSense ROS2 driver
- NumPy, OpenCV, cv_bridge
