# Manipulators

ROS2 package for Kinova Gen3 7-DOF torque control using Pinocchio for differential IK and gravity compensation. Includes object detection with a pluggable detector system and camera calibration tools. Communicates directly with the robot via Kortex API (no ros2_control drivers). Uses RealSense ROS2 driver for camera I/O.

## Architecture

```
keyboard_teleop ──/target_pose──> control_node ──torques──> Kinova (UDP @ 400Hz)
                  /gripper_cmd──┘      |
                                       |── /joint_states
                                       └── /ee_pose

realsense2_camera ──/color/image_raw──> object_detection_node ──/detected_object_point──> (policy)
                    /aligned_depth/──→         |
                    /camera_info────→          └── /detection_image
```

The control node runs a single controller (defined by launch file), with a fixed lifecycle:

```
Startup:  connect → clear faults → home → low-level servoing → torque mode → control loop
Shutdown: stop loop → position mode → high-level servoing → home → disconnect
```

## Package Structure

```
src/
├── hardware.py                  # KinovaHardware — Kortex TCP/UDP I/O + gripper
├── robot_model.py               # Pinocchio wrapper — gravity, Jacobian, FK (7-DOF reduced model)
├── diff_ik_controller.py        # Diff-IK + damped pseudoinverse + gravity comp → joint torques
├── control_node.py              # Main ROS2 node — startup/loop/shutdown
├── keyboard_teleop.py           # Keyboard → /target_pose + /gripper_command
├── object_detection_node.py     # ROS2 node — camera subscription, detection, 3D projection
├── utility.py                   # Quaternion, rotation, pose error helpers
└── detectors/
    ├── base.py                  # DetectorBase protocol + Detection dataclass
    └── color_detector.py        # HSV color-based detection

scripts/
├── capture_images.py            # Capture calibration images from camera topic
├── calibrate_intrinsics.py      # Chessboard → camera matrix + distortion → YAML
└── calibrate_extrinsics.py      # Camera-to-robot transform → YAML

config/
├── kinova_gen3.yaml             # Robot + controller parameters
├── camera.yaml                  # Camera intrinsics + extrinsics (generated by scripts)
└── detection.yaml               # Detector type, HSV ranges, crop, depth range
```

## Usage

```bash
# Build
cd ~/manipulators/ros2_ws
colcon build --packages-select manipulators
source install/setup.bash

# Run with diff-IK + keyboard teleop
ros2 launch manipulators diff_ik.launch.py robot_ip:=192.168.1.10

# Emergency stop (from another terminal)
ros2 service call /e_stop std_srvs/srv/Trigger
```

## Topics

### Control

| Topic | Type | Direction | Description |
|-------|------|-----------|-------------|
| `/target_pose` | `geometry_msgs/PoseStamped` | Input | Desired EE pose |
| `/gripper_command` | `std_msgs/Float64` | Input | Gripper target (0.0=open, 1.0=closed) |
| `/joint_states` | `sensor_msgs/JointState` | Output | Joint positions, velocities, torques |
| `/ee_pose` | `geometry_msgs/PoseStamped` | Output | Current EE pose |

### Detection

| Topic | Type | Direction | Description |
|-------|------|-----------|-------------|
| `/detected_object_point` | `geometry_msgs/PointStamped` | Output | Object position in robot frame |
| `/detection_image` | `sensor_msgs/Image` | Output | Annotated camera image |

## Services

| Service | Type | Description |
|---------|------|-------------|
| `/e_stop` | `std_srvs/Trigger` | Emergency stop — kills torque mode immediately |

## Configuration

### Control — `config/kinova_gen3.yaml`

- `robot_ip` — Kinova IP address
- `home_position_deg` — Joint angles for home position (degrees, Kinova 0-360 convention)
- `control_rate_hz` — Control loop frequency (default 400 Hz)
- `kp_task` — Task-space proportional gains `[x, y, z, rx, ry, rz]`
- `kp_joint` — Joint-space position gains (per joint, default 0.0 = disabled)
- `kd_joint` — Joint-space damping gains (per joint)
- `damping` — Pseudoinverse regularization
- `max_joint_velocity` — Safety clamp on diff-IK output (rad/s)
- `max_torque` — Per-joint torque limits (Nm)

### Detection — `config/detection.yaml`

- `detector_type` — Detector to use (`color`, or custom)
- `hsv_low` / `hsv_high` — HSV threshold range for color detector
- `bgr_low` / `bgr_high` — BGR pre-filter range
- `crop` — Region of interest `[x1, y1, x2, y2]` in pixels
- `min_area` — Minimum contour area to count as detection
- `depth_range` — Valid depth range in meters `[min, max]`
- `camera_calibration_file` — Path to `camera.yaml` with extrinsics

### Camera — `config/camera.yaml`

Generated by calibration scripts. Contains camera matrix, distortion coefficients, and camera-to-robot extrinsic transform (rvec/tvec).

## Keyboard Teleop Keys

```
W/S  — X forward/back        U/O — roll +/-
A/D  — Y left/right          I/K — pitch +/-
Q/E  — Z up/down             J/L — yaw +/-
G    — toggle gripper         ESC — quit
```

## Camera Calibration

See [docs/camera_calibration.md](docs/camera_calibration.md) for a detailed guide on intrinsic/extrinsic calibration theory and troubleshooting.

**Quick start:**

```bash
# 1. Start the RealSense camera
ros2 launch realsense2_camera rs_launch.py align_depth.enable:=true

# 2. Capture calibration images (SPACE to capture, ESC to quit)
ros2 run manipulators capture_images -- --output-dir ./calib_images --count 20

# 3. Compute intrinsics
python3 scripts/calibrate_intrinsics.py \
  --image-dir ./calib_images \
  --output config/camera.yaml \
  --board-size 8 6 \
  --square-size 25.0

# 4. Compute extrinsics (camera-to-robot)
#    Place chessboard at a known position relative to robot base
ros2 run manipulators calibrate_extrinsics -- \
  --camera-yaml config/camera.yaml \
  --board-size 5 3 \
  --square-size 45.0 \
  --board-to-robot-xyz -0.11 -0.01 0.0 \
  --board-to-robot-rpy 0.0 180.0 0.0
```

## Adding a New Detector

1. Create `src/detectors/my_detector.py` implementing `detect(color_image) -> List[Detection]`
2. Register it in `src/detectors/__init__.py` DETECTORS dict
3. Set `detector_type: my_detector` in `config/detection.yaml`

## Adding a New Controller

1. Create `src/new_controller.py` with a `compute(target_pos, target_quat, q, dq) -> torques` method
2. Import and instantiate it in a copy of `control_node.py` (or parameterize the existing one)
3. Add a new launch file `launch/new_controller.launch.py`

## Diagrams

See [docs/diagrams.md](docs/diagrams.md) for detailed mermaid diagrams covering:

- System architecture (control + detection)
- Startup / shutdown sequences
- Control loop (single cycle breakdown)
- E-stop sequence
- Threading model
- Module dependency graph
- Data flow and coordinate conversions
- Kinova hardware connection architecture (TCP vs UDP)
- Object detection node architecture
- Detection pipeline (single frame)
- Camera calibration workflow
- Pluggable detector class diagram
- Coordinate frame transforms (pixel → camera → robot)

## Dependencies

- ROS2 (Humble+)
- [Pinocchio](https://github.com/stack-of-tasks/pinocchio) — dynamics, Jacobians, FK
- [Kortex API](https://github.com/Kinovarobotics/kortex) — direct robot communication
- [realsense2_camera](https://github.com/IntelRealSense/realsense-ros) — RealSense ROS2 driver
- NumPy, OpenCV, cv_bridge
